{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''\n",
    "DESCRIPTION\n",
    "-----------\n",
    "    Training model and exporting trained model\n",
    "    \n",
    "RETURN\n",
    "------\n",
    "    {MODEL}.h5 : h5 file\n",
    "        Trained model\n",
    "    {MODEL-RESULT}.csv : csv file\n",
    "        The model result with probabilities, prediction label and ground truth\n",
    "\n",
    "EXPORTED FILE(s) LOCATION\n",
    "-------------------------\n",
    "    ./models/{NN or CV}/{EXPERIMENT}/{MODEL}.h5\n",
    "    ./models/{NN or CV}/{EXPERIMENT}/{MODEL-RESULT}.csv\n",
    "'''\n",
    "\n",
    "# importing default libraries\n",
    "# import os, argparse, sys\n",
    "# sys.path.append('./')\n",
    "import os, sys\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "os.chdir(ROOT_DIR)\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** PROJECT FOLDER     ,  /home/pgundogdu/projects/doc_p1_NN_with_pathways\n",
      "**** PROJECT DATA FOLDER,  /home/pgundogdu/projects/00_data/doc_p1_NN_with_pathways\n",
      "**** scripts/settings.py - PATHS IMPORTED!!!\n"
     ]
    }
   ],
   "source": [
    "# importing scripts in scripts folder\n",
    "from scripts import settings as srp\n",
    "# importing default libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, LeaveOneGroupOut, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from numba import cuda\n",
    "\n",
    "# DEFAULT VALUES for PAPER DESIGN\n",
    "epochs_default=100\n",
    "batch_default=10\n",
    "val_split=0.1\n",
    "\n",
    "rand_state = 91\n",
    "shuffle_=True\n",
    "test_size = 0.3 # train_test_split\n",
    "kf_split = 5 # KFold\n",
    "\n",
    "time_start = dt.datetime.now().time().strftime('%H:%M:%S') # = time.time() dt.datetime.now().strftime('%Y%m%d_%I%M%S%p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/NN/exper_immune/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset              = 'reference_log1p.pck'\n",
    "# bio_knowledge        = 'pbk_layer_hsa.txt'\n",
    "# NN_or_CV             = 'NN'\n",
    "# experiment           = 'exper_melanoma'\n",
    "\n",
    "dataset              = 'exper_immune_raw_sw_log1p.pck'\n",
    "bio_knowledge        = 'pbk_layer_hsa.txt'\n",
    "NN_or_CV             = 'NN'\n",
    "experiment           = 'exper_immune'\n",
    "\n",
    "\n",
    "# the output location\n",
    "loc_output = os.path.join(srp.DIR_MODELS, NN_or_CV, experiment)\n",
    "srp.define_folder(loc_=loc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE FORMAT,  pck\n",
      "Dataset cell type,  cell_type\n",
      "DC              957\n",
      "ILC             574\n",
      "NK               74\n",
      "Tcell         23484\n",
      "macrophage      503\n",
      "mixed            12\n",
      "dtype: int64\n",
      "\n",
      "Dataset shape             ,  (25604, 2090)\n",
      "Biological knowledge shape,  (2089, 93)\n",
      "\n",
      "Dataset gene order top 10              , ['a2m', 'abcb4', 'abcc2', 'abhd5', 'abi2', 'abl1', 'abl2', 'ablim1', 'ablim3', 'acaa1']\n",
      "Biological knowledge gene order top 10,  ['a2m', 'abcb4', 'abcc2', 'abhd5', 'abi2', 'abl1', 'abl2', 'ablim1', 'ablim3', 'acaa1']\n"
     ]
    }
   ],
   "source": [
    "print('FILE FORMAT, ', dataset.split('.')[1])\n",
    "\n",
    "if dataset.split('.')[1]=='pck':\n",
    "    df_raw = pd.read_pickle(os.path.join(srp.DIR_DATA_PROCESSED, experiment, dataset))\n",
    "    df_raw = pd.concat([(df_raw.iloc[:, :-1]).astype(float) ,df_raw.iloc[:, -1]], axis=1)\n",
    "else:\n",
    "    df_raw = pd.read_csv(os.path.join(srp.DIR_DATA_PROCESSED, experiment, dataset))\n",
    "\n",
    "# SORTING GENE LIST\n",
    "sort_genes = sorted(df_raw.columns[:-1])\n",
    "sort_genes.extend(df_raw.columns[-1:])\n",
    "df_raw = df_raw[sort_genes]\n",
    "\n",
    "# Importing all prior biological knowledge and combine all genes to create a common gene list\n",
    "list_gene = None\n",
    "if (bio_knowledge!=None):\n",
    "    df_bio = pd.DataFrame(pd.read_csv(os.path.join(srp.DIR_DATA_PROCESSED, bio_knowledge), index_col=0)).sort_index()\n",
    "    df_bio_filtered = df_bio.iloc[df_bio.index.isin(df_raw.columns), :]\n",
    "    \n",
    "\n",
    "print('Dataset cell type, ', df_raw.groupby('cell_type').size())\n",
    "print('\\nDataset shape             , ', df_raw.shape)\n",
    "print('Biological knowledge shape, ', df_bio_filtered.shape)\n",
    "\n",
    "print('\\nDataset gene order top 10              ,', list(df_raw.columns[:10]))\n",
    "print('Biological knowledge gene order top 10, ', list(df_bio_filtered.index[:10].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25604, 2089)\n",
      "(25604, 1)\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "X = df_raw.iloc[:, :-1].values\n",
    "y = df_raw.iloc[:, -1:].values\n",
    "y_ohe = ohe.fit_transform(y).toarray()\n",
    "# groups = y.reshape(1,-1)[0]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# print(groups.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\" # Stop training when `val_loss` is no longer improving\n",
    "                                           , min_delta=1e-5   # \"no longer improving\" being defined as \"no better than 1e-5 less\"\n",
    "                                           , patience=3       # \"no longer improving\" being further defined as \"for at least 3 epochs\"\n",
    "                                           , verbose=1 ) ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "kf = KFold(n_splits=kf_split, shuffle=shuffle_ , random_state=rand_state)\n",
    "\n",
    "print('KFold split applied!! The number of KFold is {}'.format(kf.get_n_splits()))\n",
    "for train_index, test_index in kf.split(X, y): # so.split(X, y)\n",
    "    print(train_index, len(train_index))\n",
    "    \n",
    "    model = srp.nn.proposed_NN(X=X, y=y , bio_layer=df_bio_filtered , design_type='bio')\n",
    "    \n",
    "    model.fit(train_X, train_y\n",
    "              , epochs=epochs_\n",
    "              , batch_size=size_batch\n",
    "              , verbose=1\n",
    "              , callbacks=callbacks\n",
    "              , validation_split=val_split)\n",
    "    y_pred = model.predict(test_X)\n",
    "    \n",
    "    del(model)\n",
    "    \n",
    "    X_train.append(X[train_index])\n",
    "    X_test.append(X[test_index])\n",
    "    y_train.append(y_ohe[train_index])\n",
    "    y_test.append(y_ohe[test_index])\n",
    "    \n",
    "\n",
    "    \n",
    "split='KFold'\n",
    "# split='train_test_split'\n",
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "if split == 'KFold':\n",
    "\n",
    "    kf = KFold(n_splits=kf_split, shuffle=shuffle_ , random_state=rand_state)\n",
    "\n",
    "    print('KFold split applied!! The number of KFold is {}'.format(kf.get_n_splits()))\n",
    "    for train_index, test_index in kf.split(X, y): # so.split(X, y)\n",
    "        print(train_index, len(train_index))\n",
    "\n",
    "        X_train.append(X[train_index])\n",
    "        X_test.append(X[test_index])\n",
    "        y_train.append(y_ohe[train_index])\n",
    "        y_test.append(y_ohe[test_index])\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "\n",
    "elif split=='train_test_split':\n",
    "    print('train_test_split split applied! Test size is, ', test_size)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y\n",
    "                                                    , test_size=test_size\n",
    "                                                    , shuffle=shuffle_\n",
    "                                                    , random_state=rand_state)\n",
    "    \n",
    "    X_train.append(Xtrain)\n",
    "    X_test.append(Xtest)\n",
    "    y_train.append(ohe.transform(ytrain).toarray())\n",
    "    y_test.append(ohe.transform(ytest).toarray())\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time_start = dt.datetime.now().time().strftime('%H:%M:%S') # = time.time()\n",
    "print('started!!!     ', time_start)\n",
    "\n",
    "time_end  = dt.datetime.now().time().strftime('%H:%M:%S') # = time.time()\n",
    "print('\\nELAPSED TIME, ', (dt.datetime.strptime(time_end,'%H:%M:%S') - dt.datetime.strptime(time_start,'%H:%M:%S')))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 1, 2])\n",
    "groups = np.array([1, 1, 2, 2])\n",
    "logo = LeaveOneGroupOut()\n",
    "logo.get_n_splits(X, y, groups)\n",
    "\n",
    "logo.get_n_splits(groups=groups)  # 'groups' is always required\n",
    "\n",
    "print(logo)\n",
    "\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_test_split split applied! Test size is,  0.3\n",
      "(1, 17922, 2089)\n"
     ]
    }
   ],
   "source": [
    "split='train_test_split'\n",
    "# split='train_test_split'\n",
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "if split == 'KFold':\n",
    "\n",
    "    kf = KFold(n_splits=kf_split, shuffle=shuffle_ , random_state=rand_state)\n",
    "\n",
    "    print('KFold split applied!! The number of KFold is {}'.format(kf.get_n_splits()))\n",
    "    for train_index, test_index in kf.split(X, y): # so.split(X, y)\n",
    "        print(train_index, len(train_index))\n",
    "\n",
    "        X_train.append(X[train_index])\n",
    "        X_test.append(X[test_index])\n",
    "        y_train.append(y_ohe[train_index])\n",
    "        y_test.append(y_ohe[test_index])\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "\n",
    "elif split=='train_test_split':\n",
    "    print('train_test_split split applied! Test size is, ', test_size)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y\n",
    "                                                    , test_size=test_size\n",
    "                                                    , shuffle=shuffle_\n",
    "                                                    , random_state=rand_state)\n",
    "    \n",
    "    X_train.append(Xtrain)\n",
    "    X_test.append(Xtest)\n",
    "    y_train.append(ohe.transform(ytrain).toarray())\n",
    "    y_test.append(ohe.transform(ytest).toarray())\n",
    "\n",
    "    print(np.array(X_train).shape)\n",
    "    \n",
    "    \n",
    "elif split=='LOGO':\n",
    "    logo = LeaveOneGroupOut()\n",
    "    for train_index, test_index in logo.split(X, y, y.reshape(1,-1)[0]):\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train.append(X[train_index])\n",
    "        X_test.append(X[test_index])\n",
    "        y_train.append(y_ohe[train_index])\n",
    "        y_test.append(y_ohe[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "  -> Network designed with prior biological knowledge with 93 nodes in first hidden layer.\n",
      "------------- NETWORK DESIGN - ARGUMENTS -------------\n",
      "-- X.shape                  , (25604, 2089)\n",
      "-- y.shape                  , (25604, 1)\n",
      "-- bio_layer.shape          , (2089, 93)\n",
      "-- design_type              , bio\n",
      "------------- NETWORK DESIGN - CALCULATED -------------\n",
      "-- input_size               , 2089\n",
      "-- first_hidden_layer_size  , 93\n",
      "-- size_output_layer        , 6\n",
      "\n",
      "\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "first_hidden_layer_size 93\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 93)                194370    \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 6)                 564       \n",
      "=================================================================\n",
      "Total params: 194,934\n",
      "Trainable params: 194,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1605/1613 [============================>.] - ETA: 0s - loss: 0.0363 - accuracy: 0.9937INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1613/1613 [==============================] - 12s 7ms/step - loss: 0.0362 - accuracy: 0.9937 - val_loss: 0.0030 - val_accuracy: 0.9989\n",
      "Epoch 2/100\n",
      "1613/1613 [==============================] - 10s 6ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 3.5266e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "1613/1613 [==============================] - 10s 6ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0035 - val_accuracy: 0.9989\n",
      "Epoch 4/100\n",
      "1613/1613 [==============================] - 10s 6ms/step - loss: 0.0164 - accuracy: 0.9994 - val_loss: 8.7572e-04 - val_accuracy: 0.9994\n",
      "Epoch 5/100\n",
      "1613/1613 [==============================] - 10s 6ms/step - loss: 3.3347e-05 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 0.9994\n",
      "Epoch 00005: early stopping\n",
      "  -> Network designed with prior biological knowledge with 93 nodes in first hidden layer.\n",
      "------------- NETWORK DESIGN - ARGUMENTS -------------\n",
      "-- X.shape                  , (25604, 2089)\n",
      "-- y.shape                  , (25604, 1)\n",
      "-- bio_layer.shape          , (2089, 93)\n",
      "-- design_type              , bio\n",
      "------------- NETWORK DESIGN - CALCULATED -------------\n",
      "-- input_size               , 2089\n",
      "-- first_hidden_layer_size  , 93\n",
      "-- size_output_layer        , 6\n",
      "\n",
      "\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "first_hidden_layer_size 93\n",
      "second_layer applied!! \n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (Dense)               (None, 93)                194370    \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 100)               9400      \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 204,376\n",
      "Trainable params: 204,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n",
      "1613/1613 [==============================] - 13s 8ms/step - loss: 0.0458 - accuracy: 0.9919 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "1613/1613 [==============================] - 11s 7ms/step - loss: 0.0146 - accuracy: 0.9973 - val_loss: 0.0048 - val_accuracy: 0.9983\n",
      "Epoch 3/100\n",
      "1613/1613 [==============================] - 10s 6ms/step - loss: 0.0119 - accuracy: 0.9988 - val_loss: 0.0016 - val_accuracy: 0.9989\n",
      "Epoch 4/100\n",
      "1613/1613 [==============================] - 11s 7ms/step - loss: 3.4759e-04 - accuracy: 0.9999 - val_loss: 0.0037 - val_accuracy: 0.9983\n",
      "Epoch 5/100\n",
      "1613/1613 [==============================] - 11s 7ms/step - loss: 1.0391e-05 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 0.9983\n",
      "Epoch 6/100\n",
      "1613/1613 [==============================] - 11s 7ms/step - loss: 5.1401e-06 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9983\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "df_nn = pd.DataFrame()\n",
    "for i in range(len(X_train)):\n",
    "    print(len(X_train))\n",
    "    time_start = dt.datetime.now().time().strftime('%H:%M:%S') # = time.time()\n",
    "    model_a1 = srp.nn.proposed_NN(X=X, y=y, bio_layer=df_bio_filtered, design_type='bio')\n",
    "    model_a1.fit(X_train[i], y_train[i]\n",
    "              , epochs=epochs_default\n",
    "              , batch_size=batch_default\n",
    "              , verbose=1\n",
    "              , callbacks=callbacks\n",
    "              , validation_split=val_split)\n",
    "#     y_pred_a1 = model_a1.predict(X_test[i])\n",
    "    \n",
    "#     print('model deleted!!')\n",
    "#     del(model_a1)\n",
    "    \n",
    "#     df_proba = pd.DataFrame(y_pred_a1, columns=list(pd.DataFrame(ohe.categories_).iloc[0,:]))\n",
    "#     df_pred = pd.DataFrame(ohe.inverse_transform(y_pred_a1).reshape(1,-1)[0], columns=['prediction'])\n",
    "#     df_ground_truth = pd.DataFrame(ohe.inverse_transform(np.array(y_test)[i]).reshape(1,-1)[0], columns=['ground_truth'])\n",
    "#     df_nn_a1 = pd.concat([df_proba, df_pred, df_ground_truth], axis=1)\n",
    "#     df_nn_a1['design'] ='a1'\n",
    "#     df_nn_a1['index_split'] = i\n",
    "#     df_nn_a1['split'] = split\n",
    "#     df_nn = pd.concat([df_nn, df_nn_a1])\n",
    "#     time_end  = dt.datetime.now().time().strftime('%H:%M:%S') # = time.time()\n",
    "#     print('\\nELAPSED TIME, ', (dt.datetime.strptime(time_end,'%H:%M:%S') - dt.datetime.strptime(time_start,'%H:%M:%S')))\n",
    "    \n",
    "    time_start = dt.datetime.now().time().strftime('%H:%M:%S') # = time.time()\n",
    "    model_a2 = srp.nn.proposed_NN(X=X, y=y, bio_layer=df_bio_filtered, design_type='bio', second_layer=True)\n",
    "    model_a2.fit(X_train[i], y_train[i]\n",
    "              , epochs=epochs_default\n",
    "              , batch_size=batch_default\n",
    "              , verbose=1\n",
    "              , callbacks=callbacks\n",
    "              , validation_split=val_split)\n",
    "#     y_pred_a2 = model_a2.predict(X_train[i])\n",
    "#     del(model_a2)\n",
    "    \n",
    "#     df_proba = pd.DataFrame(y_pred_a2, columns=list(pd.DataFrame(ohe.categories_).iloc[0,:]))\n",
    "#     df_pred = pd.DataFrame(ohe.inverse_transform(y_pred_a2).reshape(1,-1)[0], columns=['prediction'])\n",
    "#     df_ground_truth = pd.DataFrame(ohe.inverse_transform(np.array(y_test)[i]).reshape(1,-1)[0], columns=['ground_truth'])\n",
    "#     df_nn_a2 = pd.concat([df_proba, df_pred, df_ground_truth], axis=1)\n",
    "#     df_nn_a2['design'] ='a2'\n",
    "#     df_nn_a2['index_split'] = i\n",
    "#     df_nn_a2['split'] = split\n",
    "#     df_nn = pd.concat([df_nn, df_nn_a2])\n",
    "#     time_end  = dt.datetime.now().time().strftime('%H:%M:%S') # = time.time()\n",
    "#     print('\\nELAPSED TIME, ', (dt.datetime.strptime(time_end,'%H:%M:%S') - dt.datetime.strptime(time_start,'%H:%M:%S')))\n",
    "\n",
    "# df_nn.to_csv(os.path.join(loc_output,'model_result_'+split+'.csv'), index=False)\n",
    "    model_a1.save(os.path.join(loc_output, 'model_a1_'+dataset.split('.')[0]+'_'+split+'_'+str(i)+'.h5'))\n",
    "    model_a2.save(os.path.join(loc_output, 'model_a2_'+dataset.split('.')[0]+'_'+split+'_'+str(i)+'.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "numba.cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
